---
title: "Progress Memo 2"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Donny Tou"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

::: {.callout-tip icon=false}

## Github Repo Link

[Donny's Final Project Repo](https://github.com/stat301-2-2024-winter/final-project-2-donnytou.git)

:::

```{r}
#| echo: false
library(tidyverse)
library(tidymodels)
library(here)
tidymodels_prefer()
```

# Data Overview/Changes
Collated from the Chicago Police Department (CPD)^[[City of Chicago Traffic Crashes](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if/about_data)], my dataset — in its raw from — consisted of just over 800,000 observations of collision-level data. I quickly realized in the early stages of my predictive modeling process that this was simply way too much data to work with in the context of my time- and computation-based constraints; this, coupled with an imbalance in my initial outcome variable — the number of individuals emerging injured from each traffic collision — led me to pursue the following changes since Progress Memo 1:\

1. **Outcome variable modification**: Instead of predicting the *number* of individuals emerging injured from a given collision, I now predict *whether* a given traffic collision will be injurious. This makes my predictive question a **classification**, rather than regression, problem. I made this switch so that I could preserve the injury-focused nature of my initial question and, at the same time, streamline the following data downsizing process.\
2. **Downsampling**: After throwing out missing data (~0.22% of the rows), I downsampled my dataset with respect to my new outcome variable (`injurious`) using `slice_sample()`. This reduced the number of observation from over 800,000 to about 220,000^[The number of observations in the underrepresented outcome variable class — collisions that *were* injurious — was about 110,000 in the raw dataset] and also ensured a 50-50 class balance in my dummy outcome variable.\
3. **Additional downsizing**: While my downsampling-induced shrinkage certainly helped, I felt that a 220,000-row dataset still exceeded my computational limits. I therefore further downsized my sample via random selection using `initial_split()` to end up with a final dataset exactly 20% as large as the post-downsampling dataset (~44,000 observations). This final dataset was then split into training-testing sets using an **80:20** ratio; I then concluded my data splitting by partioning the ~35,000 training observations into **5 folds repeated 3 times** to generate my resampled data.

Under normal circumstances, I probably should not throw out missing observations and downsample datasets in this manner: perhaps the missing observations are missing on a non-random basis, or perhaps there is a systematic reason behind why the factor levels are so imbalanced. Such statistical focuses, however, are outside the scope of my current predictive analysis.

# Null and Baseline Fits

To investigate my predictive classification problem, I have chosen the **`accuracy` assessment metric** for its easy interpretability as well as its compatibility with the binomial nature of my `injurious` outcome variable. I have defined my 2 basic baseline models (the first being the "true" baseline; the second being a slight upgrade) as the following:\

1. **Null model**: A simple, non-informative model defined using `null_model()` with the `parsnip` engine and `classification` mode.\
2. **Logistic regression model**: A linear model that accommodates my dummy outcome variable defined using `logistic_reg()` with the `glm` engine and `classification` mode.\

These 2 models were merged with a simple recipe: one that predicted `injurious` using `month` (of collision), `posted_speed_limit`, `weather_condition`, and `street_direction`. This recipe also removed any predictors with zero variance and 
then centered/scaled numerical variables. I then fitted the 2 baseline model workflows to my 15 resamples/folds, generating the following results:
```{r}
#| label: tbl-fit-resample
#| tbl-cap: Resample fit results for null/baseline and logistic regression models
#| echo: false
load(here("results/combined_accuracy.rda"))
combined_accuracy |>
  relocate(model) |>
  knitr::kable()
```

@tbl-fit-resample displays the fitted results of my 2 simple models, averaged across folds: using the same basic recipe, the null and logistic models accurately predicted 50% and 55-56% of the of the values respectively. More-complex models and/or better-refined recipes appear to be necessary!

# Analysis Plan and Next Steps

To prepare my predictive modeling process for a "model competition", I will introduce and define **4 additional model types**: elastic net model with the `glmnet` engine (parametric); K-nearest neighbors model with the `kknn` engine (parametric *and* tree-based); random forest model with the `ranger` engine (tree-based); and boosted tree model with the `xgboost` engine (tree-based). I will take advantage of the complexity of these models by setting tuning ranges for their various hyperparameters so that — following my model competition — I can identify not only the best-performing model type, but also the best-performing *hyperparameters* for this model type. My next steps with regards to this tuning process are:\

**1.** To select the hyperparameters to tune within each of the 4 models;\
**2.** To define the specific ranges to explore for each hyperparameter

Recipe-wise, I will define (in addition to the simple recipe I used above to fit my baseline models) **4 new recipes**: 2 variants for my parametric models (elastic net and KNN), and 2 variants for my tree-based models (random forest, boosted tree, and KNN). A *potential concern* with this featuring step is that I am given a large amount (50+) of predictors: I want to take advantage of this large amount, but only to a *reasonable extent*: I do not want to include variables that see excessive missingness (> 80%) and/or are perfectly correlated with my outcome variable, `injurious`. I also want to be *actively careful* about my feature selection and not just ignore predictors that are obviously untenable: to this end, I will carefully select my predictor variables through feature extraction using lasso regressions and also conduct a more thorough general EDA process to pinpoint predictors for my outcome variable. Using this framework, my goal is to select **~15** predictors on average for use across my 4 recipes. 
